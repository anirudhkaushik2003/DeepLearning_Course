{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e3501e",
   "metadata": {},
   "source": [
    "# Question 1 Learning long term dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871eac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run MyOtherNotebook.ipynb # run auxillary scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bff45c",
   "metadata": {},
   "source": [
    "#### Question: Describe the architecture used for LSTM and for RNN. Please also describe activation functions used, learning rate.\n",
    "\n",
    "#### RNN\n",
    "- An RNN is a recurrent neural network whose hidden state at time t is calculated using the hidden state at time t-1 in addition to the input at time t in order to \n",
    "\n",
    "  $$ h_t = \\tanh(x_t W_{ih}^T + b_{ih} + h_{t-1}W_{hh}^T + b_{hh}) $$\n",
    "  \n",
    "where $h_t$ is the hidden state at time t, $x_t$x is the input at time t, and $h_{(t-1)}$ is the hidden state of the previous layer at time t-1 or the initial hidden state at time 0. \n",
    "\n",
    "- Our network consists of an input layer of size p+1 which is the input of the RNN, we apply tanh activation (ReLU can also be used, but results are displayed with tanh). The initial hidden state is simply a zero vector. The output of the RNN layer is output, h_n where h_n is the hidden state at time n. We supply this output to a linear (Dense) layer whose output is of the shape p+1 (the predicted target, which can be x or y).\n",
    "- The input size is p+1 = 101, hidden size is 128, activation function used is tanh.\n",
    "- learning_rate used is 0.01 (can be changed, results are displayed with 0.01 as default)\n",
    "\n",
    "#### LSTM\n",
    "- LSTM is a long short term memory cell. It is an RNN but there are additional components present such as a forget gate, selective read and selective write. These operations are modelled after a 'whiteboard' such that information can be selectively erased or forgotten in order to prevent information at time t-N being completely overwritten beyond recognition, i.e., 'lost' at time t. The equation is as follows:\n",
    "    \n",
    "$$\n",
    "        \\begin{array}{ll} \\\\\n",
    "            i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "            f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "            o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "            c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "            h_t = o_t \\odot \\tanh(c_t) \\\\\n",
    "        \\end{array}\n",
    "$$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is the cell\n",
    "state at time $t$, $x_t$ is the input at time $t$, $h_{t-1}$\n",
    "is the hidden state of the layer at time $t-1$ or the initial hidden\n",
    "state at time $0$, and $i_t$, $f_t$, $g_t$,\n",
    "$o_t$ are the input, forget, cell, and output gates, respectively.\n",
    "$\\sigma$ is the sigmoid function, and $\\odot$ is the Hadamard product.\n",
    "\n",
    "- Our network consists of an input layer of size p+1 which is the input to the LSTM layer, we apply the equation as described above to our input. The initial hidden state is simply a zero vector. The layer returns the output of the LSTM and the tuple (h_n, c_n) which is the hidden state at time n and the cell state at time n. We supply this output to a linear (Dense) layer whose output size is p+1 (the predited target, x or y).\n",
    "- The input size is p+1 = 101, hidden size is 128, activation function used is tanh (ReLU can also be used, but results are displayed with tanh).\n",
    "- learning_rate used is 0.01 (can be changed, results are displayed with 0.01 as default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59eac2a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23956/685576831.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_samples(p, n_batches, batch_size):\n",
    "    x_inputs_indices = np.sort(np.random.randint(low=0, high=p-2, size=[n_batches,batch_size,p-1], dtype=int))\n",
    "    x_p_indices = np.ones((n_batches,batch_size,1),dtype=np.int32)*(p-1)\n",
    "    x_inputs_indices = np.dstack((x_p_indices,x_inputs_indices,x_p_indices))\n",
    "\n",
    "    # y indices\n",
    "    y_inputs_indices = np.sort(np.random.randint(low=0, high=p-2, size=[n_batches,batch_size,p-1], dtype=int))\n",
    "    y_p_indices = np.ones((n_batches,batch_size,1),dtype=np.int32)*(p)\n",
    "    y_inputs_indices = np.dstack((y_p_indices,y_inputs_indices,y_p_indices))\n",
    "\n",
    "    x_inputs = np.zeros((n_batches,batch_size,p+1,p+1), dtype=np.int32) # batch_size, timesteps, features\n",
    "    for k in range(n_batches):\n",
    "        for i in range(batch_size):\n",
    "            for j in range(p+1):\n",
    "                x_inputs[k][i][j][x_inputs_indices[k][i][j]] = 1\n",
    "\n",
    "    y_inputs = np.zeros((n_batches,batch_size,p+1,p+1), dtype=np.int32) # batch_size, timesteps, features\n",
    "    for k in range(n_batches):\n",
    "        for i in range(batch_size):\n",
    "            for j in range(p+1):\n",
    "                y_inputs[k][i][j][y_inputs_indices[k][i][j]] = 1\n",
    "    \n",
    "    return x_inputs, y_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a091ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs_labels(p, n_batches, batch_size):\n",
    "    x_inputs, y_inputs = generate_samples(p, n_batches, batch_size)\n",
    "    \n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for k in range(n_batches):\n",
    "        inputs.append([])\n",
    "        labels.append([])\n",
    "        for j in range(batch_size):\n",
    "            inputs[k].append(x_inputs[k][j][0:100])\n",
    "            labels[k].append(np.squeeze(x_inputs[k][j][100:101]))\n",
    "        inputs[k] = np.array(inputs[k])\n",
    "        labels[k] = np.array(labels[k])\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    inputs = torch.from_numpy(inputs)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    inputs_x,labels_x = inputs.type(torch.FloatTensor),labels.type(torch.LongTensor)\n",
    "\n",
    "\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for k in range(n_batches):\n",
    "        inputs.append([])\n",
    "        labels.append([])\n",
    "        for j in range(batch_size):\n",
    "            inputs[k].append(y_inputs[k][j][0:100])\n",
    "            labels[k].append(np.squeeze(y_inputs[k][j][100:101]))\n",
    "        inputs[k] = np.array(inputs[k])\n",
    "        labels[k] = np.array(labels[k])\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    inputs = torch.from_numpy(inputs)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    inputs_y,labels_y = inputs.type(torch.FloatTensor),labels.type(torch.LongTensor)\n",
    "    \n",
    "    return inputs_x,labels_x,inputs_y,labels_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef36e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 3200 training samples in batches of size 32 for both x and y sequences\n",
    "p = 100 # time steps\n",
    "n_batches = 100\n",
    "batch_size = 32\n",
    "\n",
    "# get inputs and labels\n",
    "inputs_x,labels_x,inputs_y,labels_y = generate_inputs_labels(p, n_batches, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a59456b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23956/926786230.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# hyper parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;31m# we have a p+1 dimensional vector for each time step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# batch_size = 32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "# hyper parameters\n",
    "input_size = p+1 # we have a p+1 dimensional vector for each time step\n",
    "hidden_size = 128\n",
    "num_epochs = 10\n",
    "# batch_size = 32\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3afbd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RecurrentNeuralNet, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size,hidden_size=hidden_size,batch_first=True)\n",
    "        self.l1 = nn.Linear(hidden_size, p+1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out,h_n = self.rnn(x)\n",
    "        out = self.l1(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a38938f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMNeuralNet, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,hidden_size=hidden_size,batch_first=True)\n",
    "        self.l1 = nn.Linear(hidden_size, p+1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out,(h_n,c_n) = self.lstm(x)\n",
    "        out = self.l1(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38867abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRUNeuralNet, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=input_size,hidden_size=hidden_size,batch_first=True)\n",
    "        self.l1 = nn.Linear(hidden_size, p+1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out,h_n = self.gru(x)\n",
    "        out = self.l1(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05405425",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = RecurrentNeuralNet(input_size, hidden_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ff8e3c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 10, step 10/100, loss = 0.0692\n",
      "epoch 1 / 10, step 20/100, loss = 0.0462\n",
      "epoch 1 / 10, step 30/100, loss = 0.0176\n",
      "epoch 1 / 10, step 40/100, loss = 0.0436\n",
      "epoch 1 / 10, step 50/100, loss = 0.0087\n",
      "epoch 1 / 10, step 60/100, loss = 0.0148\n",
      "epoch 1 / 10, step 70/100, loss = 0.0079\n",
      "epoch 1 / 10, step 80/100, loss = 0.0117\n",
      "epoch 1 / 10, step 90/100, loss = 0.0034\n",
      "epoch 1 / 10, step 100/100, loss = 0.0028\n",
      "epoch 2 / 10, step 10/100, loss = 0.0008\n",
      "epoch 2 / 10, step 20/100, loss = 0.0006\n",
      "epoch 2 / 10, step 30/100, loss = 0.0005\n",
      "epoch 2 / 10, step 40/100, loss = 0.0004\n",
      "epoch 2 / 10, step 50/100, loss = 0.0003\n",
      "epoch 2 / 10, step 60/100, loss = 0.0003\n",
      "epoch 2 / 10, step 70/100, loss = 0.0002\n",
      "epoch 2 / 10, step 80/100, loss = 0.0002\n",
      "epoch 2 / 10, step 90/100, loss = 0.0001\n",
      "epoch 2 / 10, step 100/100, loss = 0.0002\n",
      "epoch 3 / 10, step 10/100, loss = 0.0001\n",
      "epoch 3 / 10, step 20/100, loss = 0.0001\n",
      "epoch 3 / 10, step 30/100, loss = 0.0001\n",
      "epoch 3 / 10, step 40/100, loss = 0.0001\n",
      "epoch 3 / 10, step 50/100, loss = 0.0001\n",
      "epoch 3 / 10, step 60/100, loss = 0.0001\n",
      "epoch 3 / 10, step 70/100, loss = 0.0001\n",
      "epoch 3 / 10, step 80/100, loss = 0.0001\n",
      "epoch 3 / 10, step 90/100, loss = 0.0001\n",
      "epoch 3 / 10, step 100/100, loss = 0.0001\n",
      "epoch 4 / 10, step 10/100, loss = 0.0001\n",
      "epoch 4 / 10, step 20/100, loss = 0.0001\n",
      "epoch 4 / 10, step 30/100, loss = 0.0001\n",
      "epoch 4 / 10, step 40/100, loss = 0.0000\n",
      "epoch 4 / 10, step 50/100, loss = 0.0000\n",
      "epoch 4 / 10, step 60/100, loss = 0.0000\n",
      "epoch 4 / 10, step 70/100, loss = 0.0000\n",
      "epoch 4 / 10, step 80/100, loss = 0.0001\n",
      "epoch 4 / 10, step 90/100, loss = 0.0000\n",
      "epoch 4 / 10, step 100/100, loss = 0.0001\n",
      "epoch 5 / 10, step 10/100, loss = 0.0001\n",
      "epoch 5 / 10, step 20/100, loss = 0.0000\n",
      "epoch 5 / 10, step 30/100, loss = 0.0000\n",
      "epoch 5 / 10, step 40/100, loss = 0.0001\n",
      "epoch 5 / 10, step 50/100, loss = 0.0001\n",
      "epoch 5 / 10, step 60/100, loss = 0.0001\n",
      "epoch 5 / 10, step 70/100, loss = 0.0000\n",
      "epoch 5 / 10, step 80/100, loss = 0.0000\n",
      "epoch 5 / 10, step 90/100, loss = 0.0000\n",
      "epoch 5 / 10, step 100/100, loss = 0.0000\n",
      "epoch 6 / 10, step 10/100, loss = 0.0000\n",
      "epoch 6 / 10, step 20/100, loss = 0.0000\n",
      "epoch 6 / 10, step 30/100, loss = 0.0000\n",
      "epoch 6 / 10, step 40/100, loss = 0.0000\n",
      "epoch 6 / 10, step 50/100, loss = 0.0000\n",
      "epoch 6 / 10, step 60/100, loss = 0.0000\n",
      "epoch 6 / 10, step 70/100, loss = 0.0000\n",
      "epoch 6 / 10, step 80/100, loss = 0.0000\n",
      "epoch 6 / 10, step 90/100, loss = 0.0000\n",
      "epoch 6 / 10, step 100/100, loss = 0.0000\n",
      "epoch 7 / 10, step 10/100, loss = 0.0000\n",
      "epoch 7 / 10, step 20/100, loss = 0.0000\n",
      "epoch 7 / 10, step 30/100, loss = 0.0000\n",
      "epoch 7 / 10, step 40/100, loss = 0.0000\n",
      "epoch 7 / 10, step 50/100, loss = 0.0000\n",
      "epoch 7 / 10, step 60/100, loss = 0.0000\n",
      "epoch 7 / 10, step 70/100, loss = 0.0000\n",
      "epoch 7 / 10, step 80/100, loss = 0.0000\n",
      "epoch 7 / 10, step 90/100, loss = 0.0000\n",
      "epoch 7 / 10, step 100/100, loss = 0.0000\n",
      "epoch 8 / 10, step 10/100, loss = 0.0000\n",
      "epoch 8 / 10, step 20/100, loss = 0.0000\n",
      "epoch 8 / 10, step 30/100, loss = 0.0000\n",
      "epoch 8 / 10, step 40/100, loss = 0.0000\n",
      "epoch 8 / 10, step 50/100, loss = 0.0000\n",
      "epoch 8 / 10, step 60/100, loss = 0.0000\n",
      "epoch 8 / 10, step 70/100, loss = 0.0000\n",
      "epoch 8 / 10, step 80/100, loss = 0.0000\n",
      "epoch 8 / 10, step 90/100, loss = 0.0000\n",
      "epoch 8 / 10, step 100/100, loss = 0.0000\n",
      "epoch 9 / 10, step 10/100, loss = 0.0000\n",
      "epoch 9 / 10, step 20/100, loss = 0.0000\n",
      "epoch 9 / 10, step 30/100, loss = 0.0000\n",
      "epoch 9 / 10, step 40/100, loss = 0.0000\n",
      "epoch 9 / 10, step 50/100, loss = 0.0000\n",
      "epoch 9 / 10, step 60/100, loss = 0.0000\n",
      "epoch 9 / 10, step 70/100, loss = 0.0000\n",
      "epoch 9 / 10, step 80/100, loss = 0.0000\n",
      "epoch 9 / 10, step 90/100, loss = 0.0000\n",
      "epoch 9 / 10, step 100/100, loss = 0.0000\n",
      "epoch 10 / 10, step 10/100, loss = 0.0000\n",
      "epoch 10 / 10, step 20/100, loss = 0.0000\n",
      "epoch 10 / 10, step 30/100, loss = 0.0000\n",
      "epoch 10 / 10, step 40/100, loss = 0.0000\n",
      "epoch 10 / 10, step 50/100, loss = 0.0000\n",
      "epoch 10 / 10, step 60/100, loss = 0.0000\n",
      "epoch 10 / 10, step 70/100, loss = 0.0000\n",
      "epoch 10 / 10, step 80/100, loss = 0.0000\n",
      "epoch 10 / 10, step 90/100, loss = 0.0000\n",
      "epoch 10 / 10, step 100/100, loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# training_loop\n",
    "rnn_loss_lis = []\n",
    "rnn_step_lis = []\n",
    "\n",
    "rnn_num_inputs = []\n",
    "rnn_input_loss = []\n",
    "steps = 1\n",
    "n_total_steps = len(inputs_x)\n",
    "for epoch in range(num_epochs):\n",
    "    lsum = 0\n",
    "    for i in range(len(inputs_x)):\n",
    "        if np.random.choice([0,1]) == 0:\n",
    "            inp = inputs_x[i]\n",
    "            lab = labels_x[i]\n",
    "        else:\n",
    "            inp = inputs_y[i]\n",
    "            lab = labels_y[i]\n",
    "            \n",
    "        \n",
    "        # forward\n",
    "        outputs =  model_rnn(inp)\n",
    "        loss = criterion(outputs, lab)\n",
    "        rnn_num_inputs.append(steps)\n",
    "        rnn_input_loss.append(loss.item())\n",
    "        # backwards \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        lsum += loss.item()/n_total_steps\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "        \n",
    "        if (i+1)%10 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')    \n",
    "    rnn_loss_lis.append(lsum)\n",
    "    rnn_step_lis.append(epoch)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50ff7e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = LSTMNeuralNet(input_size, hidden_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3049af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 10, step 10/100, loss = 0.2705\n",
      "epoch 1 / 10, step 20/100, loss = 0.0460\n",
      "epoch 1 / 10, step 30/100, loss = 0.0498\n",
      "epoch 1 / 10, step 40/100, loss = 0.0357\n",
      "epoch 1 / 10, step 50/100, loss = 0.0466\n",
      "epoch 1 / 10, step 60/100, loss = 0.0338\n",
      "epoch 1 / 10, step 70/100, loss = 0.0303\n",
      "epoch 1 / 10, step 80/100, loss = 0.0306\n",
      "epoch 1 / 10, step 90/100, loss = 0.0146\n",
      "epoch 1 / 10, step 100/100, loss = 0.0056\n",
      "epoch 2 / 10, step 10/100, loss = 0.0305\n",
      "epoch 2 / 10, step 20/100, loss = 0.0135\n",
      "epoch 2 / 10, step 30/100, loss = 0.0188\n",
      "epoch 2 / 10, step 40/100, loss = 0.0099\n",
      "epoch 2 / 10, step 50/100, loss = 0.0037\n",
      "epoch 2 / 10, step 60/100, loss = 0.0068\n",
      "epoch 2 / 10, step 70/100, loss = 0.0079\n",
      "epoch 2 / 10, step 80/100, loss = 0.0039\n",
      "epoch 2 / 10, step 90/100, loss = 0.0016\n",
      "epoch 2 / 10, step 100/100, loss = 0.0017\n",
      "epoch 3 / 10, step 10/100, loss = 0.0009\n",
      "epoch 3 / 10, step 20/100, loss = 0.0005\n",
      "epoch 3 / 10, step 30/100, loss = 0.0005\n",
      "epoch 3 / 10, step 40/100, loss = 0.0004\n",
      "epoch 3 / 10, step 50/100, loss = 0.0003\n",
      "epoch 3 / 10, step 60/100, loss = 0.0004\n",
      "epoch 3 / 10, step 70/100, loss = 0.0003\n",
      "epoch 3 / 10, step 80/100, loss = 0.0003\n",
      "epoch 3 / 10, step 90/100, loss = 0.0003\n",
      "epoch 3 / 10, step 100/100, loss = 0.0003\n",
      "epoch 4 / 10, step 10/100, loss = 0.0002\n",
      "epoch 4 / 10, step 20/100, loss = 0.0002\n",
      "epoch 4 / 10, step 30/100, loss = 0.0002\n",
      "epoch 4 / 10, step 40/100, loss = 0.0002\n",
      "epoch 4 / 10, step 50/100, loss = 0.0002\n",
      "epoch 4 / 10, step 60/100, loss = 0.0002\n",
      "epoch 4 / 10, step 70/100, loss = 0.0002\n",
      "epoch 4 / 10, step 80/100, loss = 0.0002\n",
      "epoch 4 / 10, step 90/100, loss = 0.0002\n",
      "epoch 4 / 10, step 100/100, loss = 0.0002\n",
      "epoch 5 / 10, step 10/100, loss = 0.0001\n",
      "epoch 5 / 10, step 20/100, loss = 0.0001\n",
      "epoch 5 / 10, step 30/100, loss = 0.0001\n",
      "epoch 5 / 10, step 40/100, loss = 0.0001\n",
      "epoch 5 / 10, step 50/100, loss = 0.0001\n",
      "epoch 5 / 10, step 60/100, loss = 0.0001\n",
      "epoch 5 / 10, step 70/100, loss = 0.0001\n",
      "epoch 5 / 10, step 80/100, loss = 0.0001\n",
      "epoch 5 / 10, step 90/100, loss = 0.0001\n",
      "epoch 5 / 10, step 100/100, loss = 0.0001\n",
      "epoch 6 / 10, step 10/100, loss = 0.0001\n",
      "epoch 6 / 10, step 20/100, loss = 0.0001\n",
      "epoch 6 / 10, step 30/100, loss = 0.0001\n",
      "epoch 6 / 10, step 40/100, loss = 0.0001\n",
      "epoch 6 / 10, step 50/100, loss = 0.0001\n",
      "epoch 6 / 10, step 60/100, loss = 0.0001\n",
      "epoch 6 / 10, step 70/100, loss = 0.0001\n",
      "epoch 6 / 10, step 80/100, loss = 0.0001\n",
      "epoch 6 / 10, step 90/100, loss = 0.0001\n",
      "epoch 6 / 10, step 100/100, loss = 0.0001\n",
      "epoch 7 / 10, step 10/100, loss = 0.0001\n",
      "epoch 7 / 10, step 20/100, loss = 0.0001\n",
      "epoch 7 / 10, step 30/100, loss = 0.0001\n",
      "epoch 7 / 10, step 40/100, loss = 0.0001\n",
      "epoch 7 / 10, step 50/100, loss = 0.0001\n",
      "epoch 7 / 10, step 60/100, loss = 0.0001\n",
      "epoch 7 / 10, step 70/100, loss = 0.0001\n",
      "epoch 7 / 10, step 80/100, loss = 0.0001\n",
      "epoch 7 / 10, step 90/100, loss = 0.0001\n",
      "epoch 7 / 10, step 100/100, loss = 0.0001\n",
      "epoch 8 / 10, step 10/100, loss = 0.0001\n",
      "epoch 8 / 10, step 20/100, loss = 0.0001\n",
      "epoch 8 / 10, step 30/100, loss = 0.0001\n",
      "epoch 8 / 10, step 40/100, loss = 0.0001\n",
      "epoch 8 / 10, step 50/100, loss = 0.0001\n",
      "epoch 8 / 10, step 60/100, loss = 0.0000\n",
      "epoch 8 / 10, step 70/100, loss = 0.0000\n",
      "epoch 8 / 10, step 80/100, loss = 0.0000\n",
      "epoch 8 / 10, step 90/100, loss = 0.0001\n",
      "epoch 8 / 10, step 100/100, loss = 0.0000\n",
      "epoch 9 / 10, step 10/100, loss = 0.0000\n",
      "epoch 9 / 10, step 20/100, loss = 0.0000\n",
      "epoch 9 / 10, step 30/100, loss = 0.0000\n",
      "epoch 9 / 10, step 40/100, loss = 0.0000\n",
      "epoch 9 / 10, step 50/100, loss = 0.0000\n",
      "epoch 9 / 10, step 60/100, loss = 0.0000\n",
      "epoch 9 / 10, step 70/100, loss = 0.0000\n",
      "epoch 9 / 10, step 80/100, loss = 0.0000\n",
      "epoch 9 / 10, step 90/100, loss = 0.0000\n",
      "epoch 9 / 10, step 100/100, loss = 0.0000\n",
      "epoch 10 / 10, step 10/100, loss = 0.0000\n",
      "epoch 10 / 10, step 20/100, loss = 0.0000\n",
      "epoch 10 / 10, step 30/100, loss = 0.0000\n",
      "epoch 10 / 10, step 40/100, loss = 0.0000\n",
      "epoch 10 / 10, step 50/100, loss = 0.0000\n",
      "epoch 10 / 10, step 60/100, loss = 0.0000\n",
      "epoch 10 / 10, step 70/100, loss = 0.0000\n",
      "epoch 10 / 10, step 80/100, loss = 0.0000\n",
      "epoch 10 / 10, step 90/100, loss = 0.0000\n",
      "epoch 10 / 10, step 100/100, loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# training_loop\n",
    "lstm_loss_lis = []\n",
    "lstm_step_lis = []\n",
    "lstm_num_inputs = []\n",
    "lstm_input_loss = []\n",
    "steps = 1\n",
    "n_total_steps = len(inputs_x)\n",
    "for epoch in range(num_epochs):\n",
    "    lsum = 0\n",
    "    for i in range(len(inputs_x)):\n",
    "        if np.random.choice([0,1]) == 0:\n",
    "            inp = inputs_x[i]\n",
    "            lab = labels_x[i]\n",
    "        else:\n",
    "            inp = inputs_y[i]\n",
    "            lab = labels_y[i]\n",
    "            \n",
    "        \n",
    "        # forward\n",
    "        outputs =  model_lstm(inp)\n",
    "        loss = criterion(outputs, lab)\n",
    "        lstm_num_inputs.append(steps)\n",
    "        lstm_input_loss.append(loss.item())\n",
    "        # backwards \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        lsum += loss.item()/n_total_steps\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "        \n",
    "        if (i+1)%10 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')    \n",
    "    lstm_loss_lis.append(lsum)\n",
    "    lstm_step_lis.append(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c64a9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = GRUNeuralNet(input_size, hidden_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_gru.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0dd356b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 10, step 10/100, loss = 0.1534\n",
      "epoch 1 / 10, step 20/100, loss = 0.0636\n",
      "epoch 1 / 10, step 30/100, loss = 0.0444\n",
      "epoch 1 / 10, step 40/100, loss = 0.0358\n",
      "epoch 1 / 10, step 50/100, loss = 0.0335\n",
      "epoch 1 / 10, step 60/100, loss = 0.0264\n",
      "epoch 1 / 10, step 70/100, loss = 0.0086\n",
      "epoch 1 / 10, step 80/100, loss = 0.0129\n",
      "epoch 1 / 10, step 90/100, loss = 0.0114\n",
      "epoch 1 / 10, step 100/100, loss = 0.0082\n",
      "epoch 2 / 10, step 10/100, loss = 0.0120\n",
      "epoch 2 / 10, step 20/100, loss = 0.0048\n",
      "epoch 2 / 10, step 30/100, loss = 0.0026\n",
      "epoch 2 / 10, step 40/100, loss = 0.0017\n",
      "epoch 2 / 10, step 50/100, loss = 0.0014\n",
      "epoch 2 / 10, step 60/100, loss = 0.0008\n",
      "epoch 2 / 10, step 70/100, loss = 0.0005\n",
      "epoch 2 / 10, step 80/100, loss = 0.0004\n",
      "epoch 2 / 10, step 90/100, loss = 0.0007\n",
      "epoch 2 / 10, step 100/100, loss = 0.0006\n",
      "epoch 3 / 10, step 10/100, loss = 0.0003\n",
      "epoch 3 / 10, step 20/100, loss = 0.0002\n",
      "epoch 3 / 10, step 30/100, loss = 0.0002\n",
      "epoch 3 / 10, step 40/100, loss = 0.0003\n",
      "epoch 3 / 10, step 50/100, loss = 0.0002\n",
      "epoch 3 / 10, step 60/100, loss = 0.0002\n",
      "epoch 3 / 10, step 70/100, loss = 0.0003\n",
      "epoch 3 / 10, step 80/100, loss = 0.0002\n",
      "epoch 3 / 10, step 90/100, loss = 0.0002\n",
      "epoch 3 / 10, step 100/100, loss = 0.0002\n",
      "epoch 4 / 10, step 10/100, loss = 0.0001\n",
      "epoch 4 / 10, step 20/100, loss = 0.0002\n",
      "epoch 4 / 10, step 30/100, loss = 0.0002\n",
      "epoch 4 / 10, step 40/100, loss = 0.0002\n",
      "epoch 4 / 10, step 50/100, loss = 0.0002\n",
      "epoch 4 / 10, step 60/100, loss = 0.0001\n",
      "epoch 4 / 10, step 70/100, loss = 0.0001\n",
      "epoch 4 / 10, step 80/100, loss = 0.0001\n",
      "epoch 4 / 10, step 90/100, loss = 0.0001\n",
      "epoch 4 / 10, step 100/100, loss = 0.0001\n",
      "epoch 5 / 10, step 10/100, loss = 0.0001\n",
      "epoch 5 / 10, step 20/100, loss = 0.0001\n",
      "epoch 5 / 10, step 30/100, loss = 0.0001\n",
      "epoch 5 / 10, step 40/100, loss = 0.0001\n",
      "epoch 5 / 10, step 50/100, loss = 0.0001\n",
      "epoch 5 / 10, step 60/100, loss = 0.0001\n",
      "epoch 5 / 10, step 70/100, loss = 0.0001\n",
      "epoch 5 / 10, step 80/100, loss = 0.0001\n",
      "epoch 5 / 10, step 90/100, loss = 0.0001\n",
      "epoch 5 / 10, step 100/100, loss = 0.0001\n",
      "epoch 6 / 10, step 10/100, loss = 0.0001\n",
      "epoch 6 / 10, step 20/100, loss = 0.0001\n",
      "epoch 6 / 10, step 30/100, loss = 0.0001\n",
      "epoch 6 / 10, step 40/100, loss = 0.0001\n",
      "epoch 6 / 10, step 50/100, loss = 0.0001\n",
      "epoch 6 / 10, step 60/100, loss = 0.0001\n",
      "epoch 6 / 10, step 70/100, loss = 0.0001\n",
      "epoch 6 / 10, step 80/100, loss = 0.0001\n",
      "epoch 6 / 10, step 90/100, loss = 0.0001\n",
      "epoch 6 / 10, step 100/100, loss = 0.0001\n",
      "epoch 7 / 10, step 10/100, loss = 0.0001\n",
      "epoch 7 / 10, step 20/100, loss = 0.0001\n",
      "epoch 7 / 10, step 30/100, loss = 0.0001\n",
      "epoch 7 / 10, step 40/100, loss = 0.0001\n",
      "epoch 7 / 10, step 50/100, loss = 0.0000\n",
      "epoch 7 / 10, step 60/100, loss = 0.0000\n",
      "epoch 7 / 10, step 70/100, loss = 0.0000\n",
      "epoch 7 / 10, step 80/100, loss = 0.0000\n",
      "epoch 7 / 10, step 90/100, loss = 0.0000\n",
      "epoch 7 / 10, step 100/100, loss = 0.0000\n",
      "epoch 8 / 10, step 10/100, loss = 0.0000\n",
      "epoch 8 / 10, step 20/100, loss = 0.0000\n",
      "epoch 8 / 10, step 30/100, loss = 0.0000\n",
      "epoch 8 / 10, step 40/100, loss = 0.0000\n",
      "epoch 8 / 10, step 50/100, loss = 0.0000\n",
      "epoch 8 / 10, step 60/100, loss = 0.0000\n",
      "epoch 8 / 10, step 70/100, loss = 0.0000\n",
      "epoch 8 / 10, step 80/100, loss = 0.0000\n",
      "epoch 8 / 10, step 90/100, loss = 0.0000\n",
      "epoch 8 / 10, step 100/100, loss = 0.0000\n",
      "epoch 9 / 10, step 10/100, loss = 0.0000\n",
      "epoch 9 / 10, step 20/100, loss = 0.0000\n",
      "epoch 9 / 10, step 30/100, loss = 0.0000\n",
      "epoch 9 / 10, step 40/100, loss = 0.0000\n",
      "epoch 9 / 10, step 50/100, loss = 0.0000\n",
      "epoch 9 / 10, step 60/100, loss = 0.0000\n",
      "epoch 9 / 10, step 70/100, loss = 0.0000\n",
      "epoch 9 / 10, step 80/100, loss = 0.0000\n",
      "epoch 9 / 10, step 90/100, loss = 0.0000\n",
      "epoch 9 / 10, step 100/100, loss = 0.0000\n",
      "epoch 10 / 10, step 10/100, loss = 0.0000\n",
      "epoch 10 / 10, step 20/100, loss = 0.0000\n",
      "epoch 10 / 10, step 30/100, loss = 0.0000\n",
      "epoch 10 / 10, step 40/100, loss = 0.0000\n",
      "epoch 10 / 10, step 50/100, loss = 0.0000\n",
      "epoch 10 / 10, step 60/100, loss = 0.0000\n",
      "epoch 10 / 10, step 70/100, loss = 0.0000\n",
      "epoch 10 / 10, step 80/100, loss = 0.0000\n",
      "epoch 10 / 10, step 90/100, loss = 0.0000\n",
      "epoch 10 / 10, step 100/100, loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# training_loop\n",
    "gru_loss_lis = []\n",
    "gru_step_lis = []\n",
    "gru_num_inputs = []\n",
    "gru_input_loss = []\n",
    "steps = 1\n",
    "n_total_steps = len(inputs_x)\n",
    "for epoch in range(num_epochs):\n",
    "    lsum = 0\n",
    "    for i in range(len(inputs_x)):\n",
    "        if np.random.choice([0,1]) == 0:\n",
    "            inp = inputs_x[i]\n",
    "            lab = labels_x[i]\n",
    "        else:\n",
    "            inp = inputs_y[i]\n",
    "            lab = labels_y[i]\n",
    "            \n",
    "        \n",
    "        # forward\n",
    "        outputs =  model_gru(inp)\n",
    "        loss = criterion(outputs, lab)\n",
    "        gru_num_inputs.append(steps)\n",
    "        gru_input_loss.append(loss.item())\n",
    "        # backwards \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        lsum += loss.item()/n_total_steps\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "        \n",
    "        \n",
    "        if (i+1)%10 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')    \n",
    "    gru_loss_lis.append(lsum)\n",
    "    gru_step_lis.append(epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70a0e3",
   "metadata": {},
   "source": [
    "### Plot the number of input sequences passed through the network versus training error (for both LSTM and RNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97de8f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lstm_num_inputs[0:100],lstm_input_loss[0:100], color='r')\n",
    "plt.plot(rnn_num_inputs[0:100],rnn_input_loss[0:100],color='b')\n",
    "plt.plot(gru_num_inputs[0:100],gru_input_loss[0:100],color='g')\n",
    "\n",
    "plt.legend(['LSTM inputs vs error', 'RNN inputs vs error', 'GRU inputs vs error'])\n",
    "plt.xlabel('Number of inputs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "958138e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lstm_step_lis,lstm_loss_lis, color='r')\n",
    "plt.plot(rnn_step_lis,rnn_loss_lis,color='b')\n",
    "plt.plot(gru_step_lis,gru_loss_lis,color='g')\n",
    "\n",
    "plt.legend(['LSTM convergence', 'RNN convergence', 'GRU convergence'])\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2b41c",
   "metadata": {},
   "source": [
    "### Does RNN based model takes longer time to converge than GRU or LSTM. Show using the errorconvergence plot.\n",
    "#### Answer:\n",
    " - As per the graph shown above, RNN seems to converge the fastest, followed by GRU and lastly LSTM\n",
    " - The loss (cross entropy) also seems to follow the trend: RNN < GRU < LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5ae04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "# generate 3000 training samples in batches of size 1 for both x and y sequences\n",
    "p = 100 # time steps\n",
    "n_batches = 3000\n",
    "batch_size = 1\n",
    "num_trials = 10\n",
    "\n",
    "# get inputs and labels\n",
    "test_inputs_x,test_labels_x,test_inputs_y,test_labels_y = generate_inputs_labels(p, n_batches, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4093d0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===RNN Test===\n",
      "accuracy in trial 1 of 10: 100.0\n",
      "accuracy in trial 2 of 10: 100.0\n",
      "accuracy in trial 3 of 10: 100.0\n",
      "accuracy in trial 4 of 10: 100.0\n",
      "accuracy in trial 5 of 10: 100.0\n",
      "accuracy in trial 6 of 10: 100.0\n",
      "accuracy in trial 7 of 10: 100.0\n",
      "accuracy in trial 8 of 10: 100.0\n",
      "accuracy in trial 9 of 10: 100.0\n",
      "accuracy in trial 10 of 10: 100.0\n"
     ]
    }
   ],
   "source": [
    "# testing rnn\n",
    "print(\"===RNN Test===\")\n",
    "with torch.no_grad():\n",
    "    for trial in range(num_trials):\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i in range(len(inputs_x)):\n",
    "            if np.random.choice([0,1]) == 0:\n",
    "                inp = test_inputs_x[i]\n",
    "                lab = test_labels_x[i]\n",
    "            else:\n",
    "                inp = test_inputs_y[i]\n",
    "                lab = test_labels_y[i]\n",
    "            \n",
    "            # forward\n",
    "            outputs =  model_rnn(inp)\n",
    "            loss = criterion(outputs, lab)\n",
    "            # value, index\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            n_samples += lab.shape[1]\n",
    "            n_correct += (prediction == lab).sum().item()\n",
    "        \n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'accuracy in trial {trial+1} of {num_trials}: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84dbfd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===LSTM Test===\n",
      "accuracy in trial 1 of 10: 100.0\n",
      "accuracy in trial 2 of 10: 100.0\n",
      "accuracy in trial 3 of 10: 100.0\n",
      "accuracy in trial 4 of 10: 100.0\n",
      "accuracy in trial 5 of 10: 100.0\n",
      "accuracy in trial 6 of 10: 100.0\n",
      "accuracy in trial 7 of 10: 100.0\n",
      "accuracy in trial 8 of 10: 100.0\n",
      "accuracy in trial 9 of 10: 100.0\n",
      "accuracy in trial 10 of 10: 100.0\n"
     ]
    }
   ],
   "source": [
    "# testing lstm\n",
    "print(\"===LSTM Test===\")\n",
    "with torch.no_grad():\n",
    "    for trial in range(num_trials):\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i in range(len(inputs_x)):\n",
    "            if np.random.choice([0,1]) == 0:\n",
    "                inp = test_inputs_x[i]\n",
    "                lab = test_labels_x[i]\n",
    "            else:\n",
    "                inp = test_inputs_y[i]\n",
    "                lab = test_labels_y[i]\n",
    "            \n",
    "            # forward\n",
    "            outputs =  model_lstm(inp)\n",
    "            loss = criterion(outputs, lab)\n",
    "            # value, index\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            n_samples += lab.shape[1]\n",
    "            n_correct += (prediction == lab).sum().item()\n",
    "        \n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'accuracy in trial {trial+1} of {num_trials}: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59bc22b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===GRU Test===\n",
      "accuracy in trial 1 of 10: 100.0\n",
      "accuracy in trial 2 of 10: 100.0\n",
      "accuracy in trial 3 of 10: 100.0\n",
      "accuracy in trial 4 of 10: 100.0\n",
      "accuracy in trial 5 of 10: 100.0\n",
      "accuracy in trial 6 of 10: 100.0\n",
      "accuracy in trial 7 of 10: 100.0\n",
      "accuracy in trial 8 of 10: 100.0\n",
      "accuracy in trial 9 of 10: 100.0\n",
      "accuracy in trial 10 of 10: 100.0\n"
     ]
    }
   ],
   "source": [
    "# testing gru\n",
    "print(\"===GRU Test===\")\n",
    "with torch.no_grad():\n",
    "    for trial in range(num_trials):\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i in range(len(inputs_x)):\n",
    "            if np.random.choice([0,1]) == 0:\n",
    "                inp = test_inputs_x[i]\n",
    "                lab = test_labels_x[i]\n",
    "            else:\n",
    "                inp = test_inputs_y[i]\n",
    "                lab = test_labels_y[i]\n",
    "            \n",
    "            # forward\n",
    "            outputs =  model_gru(inp)\n",
    "            loss = criterion(outputs, lab)\n",
    "            # value, index\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            n_samples += lab.shape[1]\n",
    "            n_correct += (prediction == lab).sum().item()\n",
    "        \n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'accuracy in trial {trial+1} of {num_trials}: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54569bf",
   "metadata": {},
   "source": [
    "### Average number of wrong predictions over 10 trials\n",
    " #### RNN\n",
    "   - Prediction Accuracy is 100% over 10 trials\n",
    "   - Average number of wrong predictions is 0 over 10 trials (this has been confirmed by manually checking values and comparing labels as well)\n",
    "\n",
    "#### LSTM\n",
    "   - Prediction Accuracy is 100% over 10 trials\n",
    "   - Average number of wrong predictions is 0 over 10 trials (this has been confirmed by manually checking values and comparing labels as well)\n",
    "   \n",
    "#### GRU\n",
    "   - Prediction Accuracy is 100% over 10 trials\n",
    "   - Average number of wrong predictions is 0 over 10 trials (this has been confirmed by manually checking values and comparing labels as well)\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91de923",
   "metadata": {},
   "source": [
    "# Question 2, Detecting Temporal order\n",
    "### Run the code from here on separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2854300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d10eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "input_size = 8 # we have a p+1 dimensional vector for each time step\n",
    "hidden_size = 2\n",
    "num_epochs = 3\n",
    "learning_rate = 0.1\n",
    "n_classes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e5f13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RecurrentNeuralNet, self).__init__()\n",
    "        self.rnn1 = nn.RNN(input_size=input_size,hidden_size=hidden_size,batch_first=True)\n",
    "        self.rnn2 = nn.RNN(input_size=hidden_size,hidden_size=4,batch_first=True)\n",
    "        self.rnn3 = nn.RNN(input_size=4,hidden_size=8,batch_first=True)\n",
    "        \n",
    "        self.l1 = nn.Linear(8, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out,h_n = self.rnn1(x)\n",
    "        out,h_n = self.rnn2(out)\n",
    "        out,h_n = self.rnn3(out)\n",
    "        out = self.l1(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da692403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMNeuralNet, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size,hidden_size=hidden_size,batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size,hidden_size=4,batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=4,hidden_size=8,batch_first=True)\n",
    "        \n",
    "        self.l1 = nn.Linear(8, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out,(h_n,c_n) = self.lstm1(x)\n",
    "        out,(h_n,c_n) = self.lstm2(out)\n",
    "        out,(h_n,c_n) = self.lstm3(out)\n",
    "        \n",
    "        out = self.l1(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf801e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnRecurrentNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AttnRecurrentNeuralNet, self).__init__()\n",
    "        self.rnn1 = nn.RNN(input_size=input_size,hidden_size=hidden_size,batch_first=True)\n",
    "        self.rnn2 = nn.RNN(input_size=hidden_size,hidden_size=4,batch_first=True)\n",
    "        self.rnn3 = nn.RNN(input_size=4,hidden_size=8,batch_first=True)\n",
    "        self.cos = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.l1 = nn.Linear(8, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out,h_n = self.rnn1(x)\n",
    "        out,h_n = self.rnn2(out)\n",
    "        out,h_n = self.rnn3(out) # O_T vector\n",
    "        e = self.cos(out, x)\n",
    "\n",
    "        \n",
    "        alpha = torch.transpose(torch.div(torch.transpose(self.sig(e),0,1),torch.sum(self.sig(e),1)),0,1) # attention weights\n",
    "\n",
    "        out = self.l1((x*alpha.unsqueeze(-1)))\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83bd6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one hot encodings\n",
    "def generate_inputs_labels(n_batches, batch_size):\n",
    "    encodings = {'E' : 0, 'a' : 1, 'b' : 2, 'c' : 3, 'd' : 4, 'X' : 5, 'Y' : 6, 'B' : 7}\n",
    "    labels = ['XXX'  , 'XXY'  , 'XYX'  , 'XYY' , 'YXX'  , 'YXY'  , 'YYX'  , 'YYY' ]\n",
    "\n",
    "    input_lis = []\n",
    "    label_lis = []\n",
    "    for i in range(n_batches):\n",
    "        input_lis.append([])\n",
    "        label_lis.append([])\n",
    "\n",
    "        for k in range(batch_size):\n",
    "            input_lis[i].append([])\n",
    "            label_lis[i].append([])\n",
    "\n",
    "            seq_len = np.random.randint(100,111)\n",
    "            t1 = np.random.randint(9,20)\n",
    "            t2 = np.random.randint(32,43)\n",
    "            t3 = np.random.randint(65,76)\n",
    "            label = np.random.choice(labels)\n",
    "            if seq_len < 110:                                                     \n",
    "                for j in range(seq_len, 110):\n",
    "                    input_lis[i][k].append(F.one_hot(torch.tensor(encodings['E']), len(encodings)).numpy())\n",
    "            for j in range(seq_len):\n",
    "                if j == 0:\n",
    "                    c = 'E'\n",
    "                elif j == t1:\n",
    "                    c = label[0]\n",
    "                elif j == t2:\n",
    "                    c = label[1]\n",
    "                elif j == t3:\n",
    "                    c = label[2]\n",
    "                elif j == seq_len-1:\n",
    "                    c = 'B'\n",
    "                else:\n",
    "                    c = np.random.choice(['a','b','c','d'])\n",
    "                input_lis[i][k].append(F.one_hot(torch.tensor(encodings[c]), len(encodings)).numpy())\n",
    "            label_lis[i][k] = F.one_hot(torch.tensor(labels.index(label)), n_classes).numpy()   \n",
    "            input_lis[i][k] = np.array(input_lis[i][k])\n",
    "            label_lis[i][k] = np.array(label_lis[i][k])\n",
    "\n",
    "        input_lis[i] = np.array(input_lis[i])\n",
    "        label_lis[i] = np.array(label_lis[i])\n",
    "    input_lis = np.array(input_lis)\n",
    "    label_lis = np.array(label_lis)\n",
    "\n",
    "    input_lis = torch.from_numpy(input_lis)\n",
    "    label_lis = torch.from_numpy(label_lis)\n",
    "    input_lis,label_lis = input_lis.type(torch.FloatTensor),label_lis.type(torch.LongTensor)\n",
    "    return input_lis,label_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5770b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 500\n",
    "batch_size = 20\n",
    "\n",
    "input_lis,label_lis = generate_inputs_labels(n_batches, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1da18670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 20, 110, 8])\n"
     ]
    }
   ],
   "source": [
    "print(input_lis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a545790",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = RecurrentNeuralNet(input_size, hidden_size)\n",
    "\n",
    "# adjust network parameters\n",
    "for name,param in model_rnn.named_parameters():\n",
    "    if name == 'rnn1.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -2.0))\n",
    "    elif name == 'rnn3.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -6.0))\n",
    "    elif name == 'rnn2.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -4.0))\n",
    "    elif name == 'rnn1.bias_hh_l0' or name == 'rnn2.bias_hh_l0' or name == 'rnn3.bias_hh_l0':\n",
    "        pass\n",
    "    else:   \n",
    "        param.data = nn.parameter.Parameter(nn.init.uniform_(param, -0.1,0.1))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5658143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 3, step 100/500, loss = 0.3839\n",
      "epoch 1 / 3, step 200/500, loss = 0.3845\n",
      "epoch 1 / 3, step 300/500, loss = 0.3833\n",
      "epoch 1 / 3, step 400/500, loss = 0.3789\n",
      "epoch 1 / 3, step 500/500, loss = 0.3806\n",
      "epoch 2 / 3, step 100/500, loss = 0.3812\n",
      "epoch 2 / 3, step 200/500, loss = 0.3826\n",
      "epoch 2 / 3, step 300/500, loss = 0.3828\n",
      "epoch 2 / 3, step 400/500, loss = 0.3788\n",
      "epoch 2 / 3, step 500/500, loss = 0.3802\n",
      "epoch 3 / 3, step 100/500, loss = 0.3914\n",
      "epoch 3 / 3, step 200/500, loss = 0.4035\n",
      "epoch 3 / 3, step 300/500, loss = 0.3727\n",
      "epoch 3 / 3, step 400/500, loss = 0.3949\n",
      "epoch 3 / 3, step 500/500, loss = 0.3900\n"
     ]
    }
   ],
   "source": [
    "# training_loop\n",
    "rnn_loss_lis = []\n",
    "rnn_step_lis = []\n",
    "steps = 1\n",
    "n_total_steps = len(input_lis)\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(input_lis)):\n",
    "        \n",
    "        # forward\n",
    "        outputs =  model_rnn(input_lis[i])\n",
    "        loss = criterion(outputs, label_lis[i])\n",
    "        \n",
    "        # backwards \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        steps += 1        \n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')    \n",
    "            rnn_loss_lis.append(loss.item())\n",
    "            rnn_step_lis.append(steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1e5e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = LSTMNeuralNet(input_size, hidden_size)\n",
    "\n",
    "# adjust network parameters\n",
    "for name,param in model_lstm.named_parameters():\n",
    "    if name == 'lstm1.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -2.0))\n",
    "    elif name == 'lstm3.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -6.0))\n",
    "    elif name == 'lstm2.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -4.0))\n",
    "    elif name == 'lstm1.bias_hh_l0' or name == 'lstm2.bias_hh_l0' or name == 'lstm3.bias_hh_l0':\n",
    "        pass\n",
    "    else:   \n",
    "        param.data = nn.parameter.Parameter(nn.init.uniform_(param, -0.1,0.1))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2b8d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 3, step 100/500, loss = 0.3818\n",
      "epoch 1 / 3, step 200/500, loss = 0.3854\n",
      "epoch 1 / 3, step 300/500, loss = 0.3812\n",
      "epoch 1 / 3, step 400/500, loss = 0.3813\n",
      "epoch 1 / 3, step 500/500, loss = 0.3843\n",
      "epoch 2 / 3, step 100/500, loss = 0.3817\n",
      "epoch 2 / 3, step 200/500, loss = 0.3853\n",
      "epoch 2 / 3, step 300/500, loss = 0.3814\n",
      "epoch 2 / 3, step 400/500, loss = 0.3808\n",
      "epoch 2 / 3, step 500/500, loss = 0.3836\n",
      "epoch 3 / 3, step 100/500, loss = 0.3820\n",
      "epoch 3 / 3, step 200/500, loss = 0.3849\n",
      "epoch 3 / 3, step 300/500, loss = 0.3818\n",
      "epoch 3 / 3, step 400/500, loss = 0.3800\n",
      "epoch 3 / 3, step 500/500, loss = 0.3827\n"
     ]
    }
   ],
   "source": [
    "# training_loop\n",
    "lstm_loss_lis = []\n",
    "lstm_step_lis = []\n",
    "steps = 1\n",
    "n_total_steps = len(input_lis)\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(input_lis)):\n",
    "        \n",
    "        # forward\n",
    "        outputs =  model_lstm(input_lis[i])\n",
    "        loss = criterion(outputs, label_lis[i])\n",
    "        \n",
    "        # backwards \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        steps += 1        \n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')    \n",
    "            lstm_loss_lis.append(loss.item())\n",
    "            lstm_step_lis.append(steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2483876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_AttnRnn = AttnRecurrentNeuralNet(input_size, hidden_size)\n",
    "\n",
    "# adjust network parameters\n",
    "for name,param in model_AttnRnn.named_parameters():\n",
    "    if name == 'rnn1.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -2.0))\n",
    "    elif name == 'rnn3.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -6.0))\n",
    "    elif name == 'rnn2.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -4.0))\n",
    "    elif name == 'rnn1.bias_hh_l0' or name == 'rnn2.bias_hh_l0' or name == 'rnn3.bias_hh_l0':\n",
    "        pass\n",
    "    else:\n",
    "        param.data = nn.parameter.Parameter(nn.init.uniform_(param, -0.1,0.1))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_AttnRnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eecb934d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 3, step 100/500, loss = 4.4759\n",
      "epoch 1 / 3, step 200/500, loss = 4.2584\n",
      "epoch 1 / 3, step 300/500, loss = 4.0724\n",
      "epoch 1 / 3, step 400/500, loss = 3.8885\n",
      "epoch 1 / 3, step 500/500, loss = 3.7225\n",
      "epoch 2 / 3, step 100/500, loss = 3.5577\n",
      "epoch 2 / 3, step 200/500, loss = 3.4021\n",
      "epoch 2 / 3, step 300/500, loss = 3.1913\n",
      "epoch 2 / 3, step 400/500, loss = 3.0365\n",
      "epoch 2 / 3, step 500/500, loss = 2.9381\n",
      "epoch 3 / 3, step 100/500, loss = 2.7834\n",
      "epoch 3 / 3, step 200/500, loss = 2.5371\n",
      "epoch 3 / 3, step 300/500, loss = 2.2884\n",
      "epoch 3 / 3, step 400/500, loss = 2.2198\n",
      "epoch 3 / 3, step 500/500, loss = 2.1939\n"
     ]
    }
   ],
   "source": [
    "# training_loop\n",
    "AttnRnn_loss_lis = []\n",
    "AttnRnn_step_lis = []\n",
    "steps = 1\n",
    "n_total_steps = len(input_lis)\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(input_lis)):\n",
    "        \n",
    "        # forward\n",
    "        outputs =  model_AttnRnn(input_lis[i])\n",
    "        loss = criterion(outputs, label_lis[i])\n",
    "        \n",
    "        # backwards \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        steps += 1        \n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')    \n",
    "            AttnRnn_loss_lis.append(loss.item())\n",
    "            AttnRnn_step_lis.append(steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da7eb6e",
   "metadata": {},
   "source": [
    "### How many input sequences were generated in the training phase before it meets the stopping condition\n",
    "#### Answer:\n",
    "#### RNN\n",
    " - We ran the network with a variety of differnet batch sizes and number of batches with the initial configuration consisting of 3200 input sequences (100 batches of size 32) run for 10 epochs.\n",
    " - The loss seemed to stop reducing at 0.37 following which it fluctuated between 0.37 and 0.38\n",
    " - The model was run again with 10000 input sequences (500 batches of size 20) with the loss behaving in the same way.\n",
    " - As an additonal step, I put all the temporal values responsible for determining the sequence i.e., X,Y, at the sequence end in one trial and at the beginging in one trial with no improvements, the model seemed to learn nothing.\n",
    " - The RNN and RNN+attention model was run over 600 times with adjusted parameters and variations of test sequences, including constant sequence length, paddings at various locations, using zero padding as well as padding with the start symbol at both the beginging and the end with the same result.\n",
    " - We used the RNN from the previous question with 1, 2 and 3 layers and hidden size set to 128 followed by a linear layer with no improvement.\n",
    " - We tested with softmax activation and obtained worse results, loss stabalized at 0.9 with adhoc predictions.\n",
    " - We tested by removing the linear layer and obtained worse results.\n",
    " \n",
    "#### LSTM\n",
    " - We ran the network with a variety of differnet batch sizes and number of batches with the initial configuration consisting of 3200 input sequences (100 batches of size 32) run for 10 epochs.\n",
    " - The loss seemed to stop reducing at 0.37 following which it fluctuated between 0.37 and 0.38\n",
    " - The model was run again with 10000 input sequences (500 batches of size 20) with the loss behaving in the same way.\n",
    " \n",
    "#### RNN+Attention \n",
    " - Attention mechanism was applied by following the paper, we first calculated the $e_i$ as the cosine similarity score between input symbol embedding (one-hot) $V_i$ and the network output at time T $O_T$ as $O_T \\odot V_i$, these were calculated for each batch.\n",
    " - After this, we calculated the Attention weights by taking the sigmoid of the $e_i$ scores over the sum of all $e_i's$ over $T$, the total number of sequences  as $ \\frac{\\sigma(e_i)}{\\sum_{i=1}^{T}{\\sigma(e_i)}}$\n",
    " - Finally, before sending to a fully connected linear layer, we multipy each embedding $V_i$ with the corresponding $\\alpha_i$ and take the sum, note that this is done over an entire batch of size 20.\n",
    " - The network was run with 10000 input sequences (500 batches of size 20) for over 30 epochs, However convergence was not achieved.\n",
    " - The loss seemed to reduce constantly for the first 20 epochs after which its minimum stabilized at around 0.6, fluctuation between 0.56 and 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56dc657",
   "metadata": {},
   "source": [
    "### Plot the number of input sequences passed through the network versus training error (for RNN, LSTM and RNN+Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "255da85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lstm_step_lis,lstm_loss_lis, color='r')\n",
    "plt.plot(rnn_step_lis,rnn_loss_lis,color='b')\n",
    "plt.plot(AttnRnn_step_lis,AttnRnn_loss_lis,color='g')\n",
    "\n",
    "plt.legend(['LSTM convergence', 'RNN convergence', 'AttnRnn convergence'])\n",
    "plt.xlabel('Number of input sequences')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ca466",
   "metadata": {},
   "source": [
    "### Testing\n",
    "### Report the average number of wrong predictions on the test set in 10 different trials (for RNN, LSTM and RNN+Attention using 3000 test sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72e7a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 3000 test sequences\n",
    "n_batches = 3000\n",
    "batch_size = 1\n",
    "num_trials = 10\n",
    "test_input_lis,test_label_lis = generate_inputs_labels(n_batches, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ef592de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===RNN Test===\n",
      "accuracy in trial 1 of 10: 0.0\n",
      "accuracy in trial 2 of 10: 0.0\n",
      "accuracy in trial 3 of 10: 0.0\n",
      "accuracy in trial 4 of 10: 0.0\n",
      "accuracy in trial 5 of 10: 0.0\n",
      "accuracy in trial 6 of 10: 0.0\n",
      "accuracy in trial 7 of 10: 0.0\n",
      "accuracy in trial 8 of 10: 0.0\n",
      "accuracy in trial 9 of 10: 0.0\n",
      "accuracy in trial 10 of 10: 0.0\n"
     ]
    }
   ],
   "source": [
    "# testing rnn\n",
    "print(\"===RNN Test===\")\n",
    "with torch.no_grad():\n",
    "    for trial in range(num_trials):\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i in range(len(test_input_lis)):\n",
    "            \n",
    "            # forward\n",
    "            outputs =  model_rnn(test_input_lis[i])\n",
    "            loss = criterion(outputs, test_label_lis[i])\n",
    "            # value, index|\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            n_samples += test_label_lis[i].shape[1]\n",
    "            if (prediction == test_label_lis[i]).sum().item() == 8:\n",
    "                n_correct += (prediction == test_label_lis[i]).sum().item()\n",
    "            else:\n",
    "                n_correct += 0\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'accuracy in trial {trial+1} of {num_trials}: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7eb8684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===LSTM Test===\n",
      "accuracy in trial 1 of 10: 0.0\n",
      "accuracy in trial 2 of 10: 0.0\n",
      "accuracy in trial 3 of 10: 0.0\n",
      "accuracy in trial 4 of 10: 0.0\n",
      "accuracy in trial 5 of 10: 0.0\n",
      "accuracy in trial 6 of 10: 0.0\n",
      "accuracy in trial 7 of 10: 0.0\n",
      "accuracy in trial 8 of 10: 0.0\n",
      "accuracy in trial 9 of 10: 0.0\n",
      "accuracy in trial 10 of 10: 0.0\n"
     ]
    }
   ],
   "source": [
    "# testing lstm\n",
    "print(\"===LSTM Test===\")\n",
    "with torch.no_grad():\n",
    "    for trial in range(num_trials):\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i in range(len(test_input_lis)):\n",
    "            \n",
    "            # forward\n",
    "            outputs =  model_lstm(test_input_lis[i])\n",
    "            loss = criterion(outputs, test_label_lis[i])\n",
    "            # value, index|\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            \n",
    "            n_samples += test_label_lis[i].shape[1]\n",
    "            if (prediction == test_label_lis[i]).sum().item() == 8:\n",
    "                n_correct += (prediction == test_label_lis[i]).sum().item()\n",
    "            else:\n",
    "                n_correct += 0\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'accuracy in trial {trial+1} of {num_trials}: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c8c86f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Attention+RNN Test===\n",
      "accuracy in trial 1 of 10: 0.0\n",
      "accuracy in trial 2 of 10: 0.0\n",
      "accuracy in trial 3 of 10: 0.0\n",
      "accuracy in trial 4 of 10: 0.0\n",
      "accuracy in trial 5 of 10: 0.0\n",
      "accuracy in trial 6 of 10: 0.0\n",
      "accuracy in trial 7 of 10: 0.0\n",
      "accuracy in trial 8 of 10: 0.0\n",
      "accuracy in trial 9 of 10: 0.0\n",
      "accuracy in trial 10 of 10: 0.0\n"
     ]
    }
   ],
   "source": [
    "# testing Attention RNN\n",
    "print(\"===Attention+RNN Test===\")\n",
    "with torch.no_grad():\n",
    "    for trial in range(num_trials):\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i in range(len(test_input_lis)):\n",
    "            \n",
    "            # forward\n",
    "            outputs =  model_AttnRnn(test_input_lis[i])\n",
    "            loss = criterion(outputs, test_label_lis[i])\n",
    "            # value, index|\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            \n",
    "            n_samples += test_label_lis[i].shape[1]\n",
    "            if (prediction == test_label_lis[i]).sum().item() == 8:\n",
    "                n_correct += (prediction == test_label_lis[i]).sum().item()\n",
    "            else:\n",
    "                n_correct += 0\n",
    "\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'accuracy in trial {trial+1} of {num_trials}: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd6905",
   "metadata": {},
   "source": [
    "#### Average number of wrong predictions:\n",
    "To convert the outputs to meaningful one-hot vectors we use determine the index of the max values as follows:\n",
    "\n",
    "$$ \\_, prediction = torch.max(outputs, 1) $$\n",
    "\n",
    "#### RNN\n",
    " - Our RNN seems to predict all 0s and the accuracy is thus 0%.\n",
    " \n",
    "#### LSTM\n",
    " - Our LSTM seems to predict all 0s and the accuracy is thus 0%.\n",
    "\n",
    "#### RNN+Attention\n",
    " - Our RNN+Attention seems to predict all 0s and the accuracy is thus 0%.\n",
    "\n",
    "All of these models were run with a wide variety of hyperparameters and tested as described previously but only adhoc predictions were obtained as if the network learned nothing, convergence was never attained by either of the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
