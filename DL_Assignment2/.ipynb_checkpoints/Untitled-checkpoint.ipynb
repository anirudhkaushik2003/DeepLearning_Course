{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8da061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbae0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "input_size = 8 # we have a p+1 dimensional vector for each time step\n",
    "hidden_size = 2\n",
    "num_epochs = 10\n",
    "learning_rate = 0.1\n",
    "n_classes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b16e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RecurrentNeuralNet, self).__init__()\n",
    "        self.rnn1 = nn.RNN(input_size=input_size,hidden_size=hidden_size,batch_first=True)\n",
    "        self.rnn2 = nn.RNN(input_size=hidden_size,hidden_size=4,batch_first=True)\n",
    "        self.rnn3 = nn.RNN(input_size=4,hidden_size=8,batch_first=True)\n",
    "        \n",
    "        self.l1 = nn.Linear(8, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out,h_n = self.rnn1(x)\n",
    "        out,h_n = self.rnn2(out)\n",
    "        out,h_n = self.rnn3(out)\n",
    "        out = self.l1(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8296fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMNeuralNet, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size,hidden_size=hidden_size,batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size,hidden_size=4,batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=4,hidden_size=8,batch_first=True)\n",
    "        \n",
    "        self.l1 = nn.Linear(8, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out,(h_n,c_n) = self.lstm1(x)\n",
    "        out,(h_n,c_n) = self.lstm2(out)\n",
    "        out,(h_n,c_n) = self.lstm3(out)\n",
    "        \n",
    "        out = self.l1(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f275270",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnRecurrentNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AttnRecurrentNeuralNet, self).__init__()\n",
    "        self.rnn1 = nn.RNN(input_size=input_size,hidden_size=hidden_size,batch_first=True)\n",
    "        self.rnn2 = nn.RNN(input_size=hidden_size,hidden_size=4,batch_first=True)\n",
    "        self.rnn3 = nn.RNN(input_size=4,hidden_size=8,batch_first=True)\n",
    "        self.cos = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.l1 = nn.Linear(8, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out,h_n = self.rnn1(x)\n",
    "        out,h_n = self.rnn2(out)\n",
    "        out,h_n = self.rnn3(out) # O_T vector\n",
    "        e = self.cos(out, x)\n",
    "\n",
    "        \n",
    "        alpha = torch.transpose(torch.div(torch.transpose(self.sig(e),0,1),torch.sum(self.sig(e),1)),0,1) # attention weights\n",
    "\n",
    "        out = self.l1((x*alpha.unsqueeze(-1)))\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa17024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one hot encodings\n",
    "def generate_inputs_labels(n_batches, batch_size):\n",
    "    encodings = {'E' : 0, 'a' : 1, 'b' : 2, 'c' : 3, 'd' : 4, 'X' : 5, 'Y' : 6, 'B' : 7}\n",
    "    labels = ['XXX'  , 'XXY'  , 'XYX'  , 'XYY' , 'YXX'  , 'YXY'  , 'YYX'  , 'YYY' ]\n",
    "\n",
    "    input_lis = []\n",
    "    label_lis = []\n",
    "    for i in range(n_batches):\n",
    "        input_lis.append([])\n",
    "        label_lis.append([])\n",
    "\n",
    "        for k in range(batch_size):\n",
    "            input_lis[i].append([])\n",
    "            label_lis[i].append([])\n",
    "\n",
    "            seq_len = np.random.randint(100,111)\n",
    "            t1 = np.random.randint(9,20)\n",
    "            t2 = np.random.randint(32,43)\n",
    "            t3 = np.random.randint(65,76)\n",
    "            label = np.random.choice(labels)\n",
    "            if seq_len < 110:                                                     \n",
    "                for j in range(seq_len, 110):\n",
    "                    input_lis[i][k].append(F.one_hot(torch.tensor(encodings['E']), len(encodings)).numpy())\n",
    "            for j in range(seq_len):\n",
    "                if j == 0:\n",
    "                    c = 'E'\n",
    "                elif j == t1:\n",
    "                    c = label[0]\n",
    "                elif j == t2:\n",
    "                    c = label[1]\n",
    "                elif j == t3:\n",
    "                    c = label[2]\n",
    "                elif j == seq_len-1:\n",
    "                    c = 'B'\n",
    "                else:\n",
    "                    c = np.random.choice(['a','b','c','d'])\n",
    "                input_lis[i][k].append(F.one_hot(torch.tensor(encodings[c]), len(encodings)).numpy())\n",
    "            label_lis[i][k] = F.one_hot(torch.tensor(labels.index(label)), n_classes).numpy()   \n",
    "            input_lis[i][k] = np.array(input_lis[i][k])\n",
    "            label_lis[i][k] = np.array(label_lis[i][k])\n",
    "\n",
    "        input_lis[i] = np.array(input_lis[i])\n",
    "        label_lis[i] = np.array(label_lis[i])\n",
    "    input_lis = np.array(input_lis)\n",
    "    label_lis = np.array(label_lis)\n",
    "\n",
    "    input_lis = torch.from_numpy(input_lis)\n",
    "    label_lis = torch.from_numpy(label_lis)\n",
    "    input_lis,label_lis = input_lis.type(torch.FloatTensor),label_lis.type(torch.LongTensor)\n",
    "    return input_lis,label_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ff894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 500\n",
    "batch_size = 20\n",
    "\n",
    "input_lis,label_lis = generate_inputs_labels(n_batches, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a938a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 20, 110, 8])\n"
     ]
    }
   ],
   "source": [
    "print(input_lis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aedb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = RecurrentNeuralNet(input_size, hidden_size)\n",
    "\n",
    "# adjust network parameters\n",
    "for name,param in model_rnn.named_parameters():\n",
    "    if name == 'rnn1.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -2.0))\n",
    "    elif name == 'rnn3.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -6.0))\n",
    "    elif name == 'rnn2.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -4.0))\n",
    "    elif name == 'rnn1.bias_hh_l0' or name == 'rnn2.bias_hh_l0' or name == 'rnn3.bias_hh_l0':\n",
    "        pass\n",
    "    else:   \n",
    "        param.data = nn.parameter.Parameter(nn.init.uniform_(param, -0.1,0.1))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5bf6201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 10, step 100/500, loss = 0.3786\n",
      "epoch 1 / 10, step 200/500, loss = 0.3853\n",
      "epoch 1 / 10, step 300/500, loss = 0.3749\n",
      "epoch 1 / 10, step 400/500, loss = 0.3813\n",
      "epoch 1 / 10, step 500/500, loss = 0.3760\n",
      "epoch 2 / 10, step 100/500, loss = 0.3773\n",
      "epoch 2 / 10, step 200/500, loss = 0.3823\n",
      "epoch 2 / 10, step 300/500, loss = 0.3742\n",
      "epoch 2 / 10, step 400/500, loss = 0.3806\n",
      "epoch 2 / 10, step 500/500, loss = 0.3759\n",
      "epoch 3 / 10, step 100/500, loss = 0.3778\n",
      "epoch 3 / 10, step 200/500, loss = 0.3841\n",
      "epoch 3 / 10, step 300/500, loss = 0.3736\n",
      "epoch 3 / 10, step 400/500, loss = 0.3801\n",
      "epoch 3 / 10, step 500/500, loss = 0.3758\n",
      "epoch 4 / 10, step 100/500, loss = 0.3782\n",
      "epoch 4 / 10, step 200/500, loss = 0.3822\n",
      "epoch 4 / 10, step 300/500, loss = 0.3726\n",
      "epoch 4 / 10, step 400/500, loss = 0.3805\n",
      "epoch 4 / 10, step 500/500, loss = 0.3741\n",
      "epoch 5 / 10, step 100/500, loss = 0.3766\n",
      "epoch 5 / 10, step 200/500, loss = 0.3832\n",
      "epoch 5 / 10, step 300/500, loss = 0.4100\n",
      "epoch 5 / 10, step 400/500, loss = 0.3824\n",
      "epoch 5 / 10, step 500/500, loss = 0.3771\n",
      "epoch 6 / 10, step 100/500, loss = 0.3776\n",
      "epoch 6 / 10, step 200/500, loss = 0.3837\n",
      "epoch 6 / 10, step 300/500, loss = 0.3737\n",
      "epoch 6 / 10, step 400/500, loss = 0.3800\n",
      "epoch 6 / 10, step 500/500, loss = 0.3756\n",
      "epoch 7 / 10, step 100/500, loss = 0.3775\n",
      "epoch 7 / 10, step 200/500, loss = 0.3831\n",
      "epoch 7 / 10, step 300/500, loss = 0.3737\n",
      "epoch 7 / 10, step 400/500, loss = 0.3813\n",
      "epoch 7 / 10, step 500/500, loss = 0.3750\n",
      "epoch 8 / 10, step 100/500, loss = 0.3778\n",
      "epoch 8 / 10, step 200/500, loss = 0.3820\n",
      "epoch 8 / 10, step 300/500, loss = 0.3738\n",
      "epoch 8 / 10, step 400/500, loss = 0.3789\n",
      "epoch 8 / 10, step 500/500, loss = 0.3749\n",
      "epoch 9 / 10, step 100/500, loss = 0.3774\n",
      "epoch 9 / 10, step 200/500, loss = 0.3818\n",
      "epoch 9 / 10, step 300/500, loss = 0.3738\n",
      "epoch 9 / 10, step 400/500, loss = 0.3788\n",
      "epoch 9 / 10, step 500/500, loss = 0.3748\n",
      "epoch 10 / 10, step 100/500, loss = 0.3774\n",
      "epoch 10 / 10, step 200/500, loss = 0.3817\n",
      "epoch 10 / 10, step 300/500, loss = 0.3738\n",
      "epoch 10 / 10, step 400/500, loss = 0.3787\n",
      "epoch 10 / 10, step 500/500, loss = 0.3748\n"
     ]
    }
   ],
   "source": [
    "# training_loop\n",
    "rnn_loss_lis = []\n",
    "rnn_step_lis = []\n",
    "steps = 1\n",
    "n_total_steps = len(input_lis)\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(input_lis)):\n",
    "        \n",
    "        # forward\n",
    "        outputs =  model_rnn(input_lis[i])\n",
    "        loss = criterion(outputs, label_lis[i])\n",
    "        \n",
    "        # backwards \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        steps += 1        \n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')    \n",
    "            rnn_loss_lis.append(loss.item())\n",
    "            rnn_step_lis.append(steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cb1f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = LSTMNeuralNet(input_size, hidden_size)\n",
    "\n",
    "# adjust network parameters\n",
    "for name,param in model_lstm.named_parameters():\n",
    "    if name == 'lstm1.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -2.0))\n",
    "    elif name == 'lstm3.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -6.0))\n",
    "    elif name == 'lstm2.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -4.0))\n",
    "    elif name == 'lstm1.bias_hh_l0' or name == 'lstm2.bias_hh_l0' or name == 'lstm3.bias_hh_l0':\n",
    "        pass\n",
    "    else:   \n",
    "        param.data = nn.parameter.Parameter(nn.init.uniform_(param, -0.1,0.1))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff97e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 10, step 100/500, loss = 0.3771\n",
      "epoch 1 / 10, step 200/500, loss = 0.3883\n",
      "epoch 1 / 10, step 300/500, loss = 0.3755\n",
      "epoch 1 / 10, step 400/500, loss = 0.3819\n",
      "epoch 1 / 10, step 500/500, loss = 0.3762\n",
      "epoch 2 / 10, step 100/500, loss = 0.3769\n",
      "epoch 2 / 10, step 200/500, loss = 0.3853\n",
      "epoch 2 / 10, step 300/500, loss = 0.3736\n",
      "epoch 2 / 10, step 400/500, loss = 0.3810\n",
      "epoch 2 / 10, step 500/500, loss = 0.3749\n",
      "epoch 3 / 10, step 100/500, loss = 0.3769\n",
      "epoch 3 / 10, step 200/500, loss = 0.3839\n",
      "epoch 3 / 10, step 300/500, loss = 0.3732\n",
      "epoch 3 / 10, step 400/500, loss = 0.3804\n",
      "epoch 3 / 10, step 500/500, loss = 0.3747\n",
      "epoch 4 / 10, step 100/500, loss = 0.3769\n",
      "epoch 4 / 10, step 200/500, loss = 0.3832\n",
      "epoch 4 / 10, step 300/500, loss = 0.3733\n",
      "epoch 4 / 10, step 400/500, loss = 0.3798\n",
      "epoch 4 / 10, step 500/500, loss = 0.3748\n",
      "epoch 5 / 10, step 100/500, loss = 0.3769\n",
      "epoch 5 / 10, step 200/500, loss = 0.3827\n",
      "epoch 5 / 10, step 300/500, loss = 0.3730\n",
      "epoch 5 / 10, step 400/500, loss = 0.3795\n",
      "epoch 5 / 10, step 500/500, loss = 0.3748\n",
      "epoch 6 / 10, step 100/500, loss = 0.3771\n",
      "epoch 6 / 10, step 200/500, loss = 0.3828\n",
      "epoch 6 / 10, step 300/500, loss = 0.3732\n",
      "epoch 6 / 10, step 400/500, loss = 0.3794\n",
      "epoch 6 / 10, step 500/500, loss = 0.3749\n"
     ]
    }
   ],
   "source": [
    "# training_loop\n",
    "lstm_loss_lis = []\n",
    "lstm_step_lis = []\n",
    "steps = 1\n",
    "n_total_steps = len(input_lis)\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(input_lis)):\n",
    "        \n",
    "        # forward\n",
    "        outputs =  model_lstm(input_lis[i])\n",
    "        loss = criterion(outputs, label_lis[i])\n",
    "        \n",
    "        # backwards \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        steps += 1        \n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')    \n",
    "            lstm_loss_lis.append(loss.item())\n",
    "            lstm_step_lis.append(steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_AttnRnn = AttnRecurrentNeuralNet(input_size, hidden_size)\n",
    "\n",
    "# adjust network parameters\n",
    "for name,param in model_AttnRnn.named_parameters():\n",
    "    if name == 'rnn1.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -2.0))\n",
    "    elif name == 'rnn3.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -6.0))\n",
    "    elif name == 'rnn2.bias_ih_l0':\n",
    "        param.data = nn.parameter.Parameter(nn.init.constant_(param, -4.0))\n",
    "    elif name == 'rnn1.bias_hh_l0' or name == 'rnn2.bias_hh_l0' or name == 'rnn3.bias_hh_l0':\n",
    "        pass\n",
    "    else:\n",
    "        param.data = nn.parameter.Parameter(nn.init.uniform_(param, -0.1,0.1))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_AttnRnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2224997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loop\n",
    "AttnRnn_loss_lis = []\n",
    "AttnRnn_step_lis = []\n",
    "steps = 1\n",
    "n_total_steps = len(input_lis)\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(input_lis)):\n",
    "        \n",
    "        # forward\n",
    "        outputs =  model_AttnRnn(input_lis[i])\n",
    "        loss = criterion(outputs, label_lis[i])\n",
    "        \n",
    "        # backwards \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        steps += 1        \n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')    \n",
    "            AttnRnn_loss_lis.append(loss.item())\n",
    "            AttnRnn_step_lis.append(steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c8a3e",
   "metadata": {},
   "source": [
    "### How many input sequences were generated in the training phase before it meets the stopping condition\n",
    "#### Answer:\n",
    "#### RNN\n",
    " - We ran the network with a variety of differnet batch sizes and number of batches with the initial configuration consisting of 3200 input sequences (100 batches of size 32) run for 10 epochs.\n",
    " - The loss seemed to stop reducing at 0.37 following which it fluctuated between 0.37 and 0.38\n",
    " - The model was run again with 10000 input sequences (500 batches of size 20) with the loss behaving in the same way.\n",
    " - As an additonal step, I put all the temporal values responsible for determining the sequence i.e., X,Y, at the sequence end in one trial and at the beginging in one trial with no improvements, the model seemed to learn nothing.\n",
    " - The RNN and RNN+attention model was run over 600 times with adjusted parameters and variations of test sequences, including constant sequence length, paddings at various locations, using zero padding as well as padding with the start symbol at both the beginging and the end with the same result.\n",
    " - We used the RNN from the previous question with 1, 2 and 3 layers and hidden size set to 128 followed by a linear layer with no improvement.\n",
    " - We tested with softmax activation and obtained worse results, loss stabalized at 0.9 with adhoc predictions.\n",
    " - We tested by removing the linear layer and obtained worse results.\n",
    " \n",
    "#### LSTM\n",
    " - We ran the network with a variety of differnet batch sizes and number of batches with the initial configuration consisting of 3200 input sequences (100 batches of size 32) run for 10 epochs.\n",
    " - The loss seemed to stop reducing at 0.37 following which it fluctuated between 0.37 and 0.38\n",
    " - The model was run again with 10000 input sequences (500 batches of size 20) with the loss behaving in the same way.\n",
    " \n",
    "#### RNN+Attention \n",
    " - Attention mechanism was applied by following the paper, we first calculated the $e_i$ as the cosine similarity score between input symbol embedding (one-hot) $V_i$ and the network output at time T $O_T$ as $O_T \\odot V_i$, these were calculated for each batch.\n",
    " - After this, we calculated the Attention weights by taking the sigmoid of the $e_i$ scores over the sum of all $e_i's$ over $T$, the total number of sequences  as $ \\frac{\\sigma(e_i)}{\\sum_{i=1}^{T}{\\sigma(e_i)}}$\n",
    " - Finally, before sending to a fully connected linear layer, we multipy each embedding $V_i$ with the corresponding $\\alpha_i$ and take the sum, note that this is done over an entire batch of size 20.\n",
    " - The network was run with 10000 input sequences (500 batches of size 20) for over 30 epochs, However convergence was not achieved.\n",
    " - The loss seemed to reduce constantly for the first 20 epochs after which its minimum stabilized at around 0.6, fluctuation between 0.56 and 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512c211",
   "metadata": {},
   "source": [
    "### Plot the number of input sequences passed through the network versus training error (for RNN, LSTM and RNN+Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lstm_step_lis,lstm_loss_lis, color='r')\n",
    "plt.plot(rnn_step_lis,rnn_loss_lis,color='b')\n",
    "plt.plot(AttnRnn_step_lis,AttnRnn_loss_lis,color='g')\n",
    "\n",
    "plt.legend(['LSTM convergence', 'RNN convergence', 'AttnRnn convergence'])\n",
    "plt.xlabel('Number of input sequences')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7734778",
   "metadata": {},
   "source": [
    "### Testing\n",
    "### Report the average number of wrong predictions on the test set in 10 different trials (for RNN, LSTM and RNN+Attention using 3000 test sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea43452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 3000 test sequences\n",
    "n_batches = 3000\n",
    "batch_size = 1\n",
    "num_trials = 10\n",
    "test_input_lis,test_label_lis = generate_inputs_labels(n_batches, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing rnn\n",
    "print(\"===RNN Test===\")\n",
    "with torch.no_grad():\n",
    "    for trial in range(num_trials):\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i in range(len(test_input_lis)):\n",
    "            \n",
    "            # forward\n",
    "            outputs =  model_rnn(test_input_lis[i])\n",
    "            loss = criterion(outputs, test_label_lis[i])\n",
    "            # value, index|\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            n_samples += test_label_lis[i].shape[1]\n",
    "            if (prediction == test_label_lis[i]).sum().item() == 8:\n",
    "                n_correct += (prediction == test_label_lis[i]).sum().item()\n",
    "            else:\n",
    "                n_correct += 0\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'accuracy in trial {trial+1} of {num_trials}: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed60d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing lstm\n",
    "print(\"===LSTM Test===\")\n",
    "with torch.no_grad():\n",
    "    for trial in range(num_trials):\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i in range(len(test_input_lis)):\n",
    "            \n",
    "            # forward\n",
    "            outputs =  model_lstm(test_input_lis[i])\n",
    "            loss = criterion(outputs, test_label_lis[i])\n",
    "            # value, index|\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            \n",
    "            n_samples += test_label_lis[i].shape[1]\n",
    "            if (prediction == test_label_lis[i]).sum().item() == 8:\n",
    "                n_correct += (prediction == test_label_lis[i]).sum().item()\n",
    "            else:\n",
    "                n_correct += 0\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'accuracy in trial {trial+1} of {num_trials}: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbbe505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Attention RNN\n",
    "print(\"===Attention+RNN Test===\")\n",
    "with torch.no_grad():\n",
    "    for trial in range(num_trials):\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for i in range(len(test_input_lis)):\n",
    "            \n",
    "            # forward\n",
    "            outputs =  model_AttnRnn(test_input_lis[i])\n",
    "            loss = criterion(outputs, test_label_lis[i])\n",
    "            # value, index|\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            \n",
    "            n_samples += test_label_lis[i].shape[1]\n",
    "            if (prediction == test_label_lis[i]).sum().item() == 8:\n",
    "                n_correct += (prediction == test_label_lis[i]).sum().item()\n",
    "            else:\n",
    "                n_correct += 0\n",
    "\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'accuracy in trial {trial+1} of {num_trials}: {acc}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cde555f7",
   "metadata": {},
   "source": [
    "#### Average number of wrong predictions:\n",
    "To convert the outputs to meaningful one-hot vectors we use determine the index of the max values as follows:\n",
    "\n",
    "$$ \\_, prediction = torch.max(outputs, 1) $$\n",
    "\n",
    "#### RNN\n",
    " - Our RNN seems to predict all 0s and the accuracy is thus 0%.\n",
    " \n",
    "#### LSTM\n",
    " - Our LSTM seems to predict all 0s and the accuracy is thus 0%.\n",
    "\n",
    "#### RNN+Attention\n",
    " - Our RNN+Attention seems to predict all 0s and the accuracy is thus 0%.\n",
    "\n",
    "All of these models were run with a wide variety of hyperparameters and tested as described previously but only adhoc predictions were obtained as if the network learned nothing, convergence was never attained by either of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6902376d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
